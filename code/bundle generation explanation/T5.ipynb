{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:22:13.264928Z",
     "start_time": "2022-07-22T02:22:12.308204Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:30.522918Z",
     "start_time": "2022-07-22T02:28:30.503764Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "# train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "idx_intent = np.load('data/total_idx_intent.npy', allow_pickle=True).item()\n",
    "idx_titles = np.load('data/total_idx_titles.npy', allow_pickle=True).item()\n",
    "\n",
    "intents = []\n",
    "titles = []\n",
    "for k,v in idx_intent.items():\n",
    "    intents.append(v)\n",
    "    titles.append(idx_titles[k])\n",
    "    \n",
    "# split train, test set = 8:2\n",
    "test_num = int(len(intents)*0.2)\n",
    "\n",
    "train_intent = intents[:-test_num]\n",
    "train_titles = titles[:-test_num]\n",
    "test_intent = intents[-test_num:]\n",
    "test_titles = titles[-test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:34.160276Z",
     "start_time": "2022-07-22T02:28:34.151130Z"
    }
   },
   "outputs": [],
   "source": [
    "train_intent.pop(1031)\n",
    "train_titles.pop(1031)\n",
    "test_intent.pop(816)\n",
    "test_titles.pop(816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:50.190230Z",
     "start_time": "2022-07-22T02:28:36.225465Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:50.201467Z",
     "start_time": "2022-07-22T02:28:50.191463Z"
    }
   },
   "outputs": [],
   "source": [
    "title_intent = defaultdict(list)\n",
    "# encoder_max_length=512\n",
    "# decoder_max_length=32\n",
    "\n",
    "for i in range(len(train_intent)):\n",
    "    # token_titles = tokenizer(train_titles[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=encoder_max_length)\n",
    "    # title_intent['titles'].append(torch.as_tensor(token_titles.input_ids, dtype=torch.int))\n",
    "    # title_intent['attention_mask'].append(torch.as_tensor(token_titles.attention_mask, dtype=torch.int))\n",
    "    # token_labels = tokenizer(train_intent[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=decoder_max_length)\n",
    "    # title_intent['labels'].append(torch.as_tensor(token_labels.input_ids, dtype=torch.int))\n",
    "    # title_intent['decoder_attention_mask'].append(torch.as_tensor(token_labels.attention_mask, dtype=torch.int))\n",
    "    title_intent['titles'].append(train_titles[i])\n",
    "    title_intent['intents'].append(train_intent[i])\n",
    "    \n",
    "test_title_intent = defaultdict(list)\n",
    "for i in range(len(test_intent)):\n",
    "    # token_titles = tokenizer(test_titles[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=encoder_max_length)\n",
    "    # test_title_intent['titles'].append(torch.as_tensor(token_titles.input_ids, dtype=torch.int))\n",
    "    # test_title_intent['attention_mask'].append(torch.as_tensor(token_titles.attention_mask, dtype=torch.int))\n",
    "    # token_labels = tokenizer(test_intent[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=decoder_max_length)\n",
    "    # test_title_intent['labels'].append(torch.as_tensor(token_labels.input_ids, dtype=torch.int))\n",
    "    # test_title_intent['decoder_attention_mask'].append(torch.as_tensor(token_labels.attention_mask, dtype=torch.int))\n",
    "    test_title_intent['titles'].append(test_titles[i])\n",
    "    test_title_intent['intents'].append(test_intent[i])\n",
    "    \n",
    "dataset = Dataset.from_dict(title_intent)\n",
    "vali_dataset = Dataset.from_dict(test_title_intent)\n",
    "\n",
    "# dataset.set_format(\"torch\")\n",
    "# vali_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:50.205824Z",
     "start_time": "2022-07-22T02:28:50.202595Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_max_length=512\n",
    "decoder_max_length=64\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "    inputs = [prefix + doc for doc in batch[\"titles\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=encoder_max_length, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"intents\"], max_length=decoder_max_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "#     inputs = tokenizer(batch[\"titles\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "#     outputs = tokenizer(batch[\"intents\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "#     batch[\"input_ids\"] = inputs.input_ids\n",
    "#     batch[\"attention_mask\"] = inputs.attention_mask\n",
    "#   # batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "#     batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "#     batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "#   # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "#   # We have to make sure that the PAD token is ignored\n",
    "#     batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:51.231764Z",
     "start_time": "2022-07-22T02:28:50.206800Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_data = dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"titles\", \"intents\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:51.235447Z",
     "start_time": "2022-07-22T02:28:51.233147Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.set_format(\n",
    "    type=\"torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:55.448655Z",
     "start_time": "2022-07-22T02:28:55.161309Z"
    }
   },
   "outputs": [],
   "source": [
    "val_data = vali_dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"titles\", \"intents\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:28:55.668986Z",
     "start_time": "2022-07-22T02:28:55.664323Z"
    }
   },
   "outputs": [],
   "source": [
    "val_data.set_format(\n",
    "    type=\"torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:29:00.294098Z",
     "start_time": "2022-07-22T02:28:56.225419Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:29:00.530998Z",
     "start_time": "2022-07-22T02:29:00.295424Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:29:24.272336Z",
     "start_time": "2022-07-22T02:29:24.264802Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    fp16=True, \n",
    "    output_dir=\"/home/workshop/dataset/fkd/bertGeneration/t5\",\n",
    "    logging_steps=2500,\n",
    "    save_steps=10000,\n",
    "    eval_steps=2500,\n",
    "    learning_rate=7e-5,\n",
    "    num_train_epochs=3,\n",
    "    # logging_steps=1000,\n",
    "    # save_steps=500,\n",
    "    # eval_steps=7500,\n",
    "    # warmup_steps=2000,\n",
    "    # save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T02:29:02.506050Z",
     "start_time": "2022-07-22T02:29:00.585058Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    # print(pred_str)\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T03:04:10.235902Z",
     "start_time": "2022-07-22T02:29:28.154172Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T14:57:55.110962Z",
     "start_time": "2022-07-19T14:57:48.778530Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5/checkpoint-3000/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T14:57:40.227237Z",
     "start_time": "2022-07-19T14:57:40.105913Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5/checkpoint-3000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T03:12:07.360577Z",
     "start_time": "2022-07-22T03:12:07.354390Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def generate_summary(batch):\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"titles\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred_summary\"] = output_str\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T03:12:54.540263Z",
     "start_time": "2022-07-22T03:12:08.664540Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 4  # change to 64 for full evaluation\n",
    "\n",
    "results = vali_dataset.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"titles\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T03:12:54.667362Z",
     "start_time": "2022-07-22T03:12:54.541624Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_output = rouge.compute(predictions=results[\"pred_summary\"], references=results[\"intents\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T03:12:54.799752Z",
     "start_time": "2022-07-22T03:12:54.673035Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_output = rouge.compute(predictions=results[\"pred_summary\"], references=results[\"intents\"], rouge_types=[\"rouge1\"])[\"rouge1\"].mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T03:12:54.927931Z",
     "start_time": "2022-07-22T03:12:54.804931Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_output = rouge.compute(predictions=results[\"pred_summary\"], references=results[\"intents\"], rouge_types=[\"rougeL\"])[\"rougeL\"].mid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
