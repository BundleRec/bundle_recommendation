{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:49:32.468237Z",
     "start_time": "2022-07-20T01:49:31.516899Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:49:32.478123Z",
     "start_time": "2022-07-20T01:49:32.469546Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "# train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "idx_intent = np.load('data/total_idx_intent.npy', allow_pickle=True).item()\n",
    "idx_titles = np.load('data/total_idx_titles.npy', allow_pickle=True).item()\n",
    "\n",
    "intents = []\n",
    "titles = []\n",
    "for k,v in idx_intent.items():\n",
    "    intents.append(v)\n",
    "    titles.append(idx_titles[k])\n",
    "    \n",
    "# split train, test set = 8:2\n",
    "test_num = int(len(intents)*0.2)\n",
    "\n",
    "train_intent = intents[:-test_num]\n",
    "train_titles = titles[:-test_num]\n",
    "test_intent = intents[-test_num:]\n",
    "test_titles = titles[-test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:49:32.717559Z",
     "start_time": "2022-07-20T01:49:32.691193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nestle Hot Cocoa Mix Rich Chocolate - 70/0.75oz. Envelopes, Community Coffee Whole Bean Coffee, French Roast, 12-Ounce Bags (Pack of 3)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_intent.pop(1031)\n",
    "train_titles.pop(1031)\n",
    "test_intent.pop(816)\n",
    "test_titles.pop(816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:00.955035Z",
     "start_time": "2022-07-20T01:49:33.037914Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BartTokenizerFast\n",
    "tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-base\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:00.969140Z",
     "start_time": "2022-07-20T01:50:00.956474Z"
    }
   },
   "outputs": [],
   "source": [
    "title_intent = defaultdict(list)\n",
    "# encoder_max_length=512\n",
    "# decoder_max_length=32\n",
    "\n",
    "for i in range(len(train_intent)):\n",
    "    # token_titles = tokenizer(train_titles[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=encoder_max_length)\n",
    "    # title_intent['titles'].append(torch.as_tensor(token_titles.input_ids, dtype=torch.int))\n",
    "    # title_intent['attention_mask'].append(torch.as_tensor(token_titles.attention_mask, dtype=torch.int))\n",
    "    # token_labels = tokenizer(train_intent[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=decoder_max_length)\n",
    "    # title_intent['labels'].append(torch.as_tensor(token_labels.input_ids, dtype=torch.int))\n",
    "    # title_intent['decoder_attention_mask'].append(torch.as_tensor(token_labels.attention_mask, dtype=torch.int))\n",
    "    title_intent['titles'].append(train_titles[i])\n",
    "    title_intent['intents'].append(train_intent[i])\n",
    "    \n",
    "test_title_intent = defaultdict(list)\n",
    "for i in range(len(test_intent)):\n",
    "    # token_titles = tokenizer(test_titles[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=encoder_max_length)\n",
    "    # test_title_intent['titles'].append(torch.as_tensor(token_titles.input_ids, dtype=torch.int))\n",
    "    # test_title_intent['attention_mask'].append(torch.as_tensor(token_titles.attention_mask, dtype=torch.int))\n",
    "    # token_labels = tokenizer(test_intent[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=decoder_max_length)\n",
    "    # test_title_intent['labels'].append(torch.as_tensor(token_labels.input_ids, dtype=torch.int))\n",
    "    # test_title_intent['decoder_attention_mask'].append(torch.as_tensor(token_labels.attention_mask, dtype=torch.int))\n",
    "    test_title_intent['titles'].append(test_titles[i])\n",
    "    test_title_intent['intents'].append(test_intent[i])\n",
    "    \n",
    "dataset = Dataset.from_dict(title_intent)\n",
    "vali_dataset = Dataset.from_dict(test_title_intent)\n",
    "\n",
    "# dataset.set_format(\"torch\")\n",
    "# vali_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:00.974105Z",
     "start_time": "2022-07-20T01:50:00.970624Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_max_length=512\n",
    "decoder_max_length=64\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "    inputs = tokenizer(batch[\"titles\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    outputs = tokenizer(batch[\"intents\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "  # batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "  # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:02.580294Z",
     "start_time": "2022-07-20T01:50:00.975345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c96c07c8524cdeba822090fd3f2a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1089 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_data = dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"titles\", \"intents\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:02.583949Z",
     "start_time": "2022-07-20T01:50:02.581497Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:03.010975Z",
     "start_time": "2022-07-20T01:50:02.585091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb56abf974994bfa8b81f73d0c3c3783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data = vali_dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"titles\", \"intents\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:03.014311Z",
     "start_time": "2022-07-20T01:50:03.011973Z"
    }
   },
   "outputs": [],
   "source": [
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:10.627292Z",
     "start_time": "2022-07-20T01:50:03.016000Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:10.911790Z",
     "start_time": "2022-07-20T01:50:10.628329Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True, \n",
    "    output_dir=\"/home/workshop/dataset/fkd/bertGeneration/bart/\",\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    eval_steps=500,\n",
    "    learning_rate=0.00002,\n",
    "    num_train_epochs=3,\n",
    "    # logging_steps=1000,\n",
    "    # save_steps=500,\n",
    "    # eval_steps=7500,\n",
    "    # warmup_steps=2000,\n",
    "    # save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T01:50:12.420058Z",
     "start_time": "2022-07-20T01:50:10.912828Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    # print(pred_str)\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:01:33.084716Z",
     "start_time": "2022-07-20T01:50:12.421131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 4355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3267\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3267' max='3267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3267/3267 11:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.073200</td>\n",
       "      <td>2.468399</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.673500</td>\n",
       "      <td>2.368719</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.319800</td>\n",
       "      <td>2.374057</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.256800</td>\n",
       "      <td>2.299472</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.058200</td>\n",
       "      <td>2.309595</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.989600</td>\n",
       "      <td>2.286724</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1087\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1087\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-1000\n",
      "Configuration saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-1000/config.json\n",
      "Model weights saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1087\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1087\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-2000\n",
      "Configuration saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-2000/config.json\n",
      "Model weights saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-2000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1087\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1087\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000\n",
      "Configuration saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/config.json\n",
      "Model weights saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3267, training_loss=2.3597072464703857, metrics={'train_runtime': 677.9933, 'train_samples_per_second': 19.27, 'train_steps_per_second': 4.819, 'total_flos': 3983103413452800.0, 'train_loss': 2.3597072464703857, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:02:19.961693Z",
     "start_time": "2022-07-20T02:02:18.428878Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:02:21.544338Z",
     "start_time": "2022-07-20T02:02:21.481177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/added_tokens.json. We won't load it.\n",
      "loading file /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/vocab.json\n",
      "loading file /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/merges.txt\n",
      "loading file /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/special_tokens_map.json\n",
      "loading file /home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizerFast.from_pretrained(\"/home/workshop/dataset/fkd/bertGeneration/bart/checkpoint-3000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:02:21.937414Z",
     "start_time": "2022-07-20T02:02:21.930999Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_summary(batch):\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"titles\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred_summary\"] = output_str\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:02:53.972523Z",
     "start_time": "2022-07-20T02:02:22.521727Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87882f0dc3cf4a26a45069039954e44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 4  # change to 64 for full evaluation\n",
    "\n",
    "results = vali_dataset.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"titles\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:06:59.715751Z",
     "start_time": "2022-07-20T02:06:59.584035Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_output = rouge.compute(predictions=results[\"pred_summary\"], references=results[\"intents\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:07:00.461315Z",
     "start_time": "2022-07-20T02:07:00.457011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0779, 0.0694, 0.0691)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rouge_output.precision, 4),round(rouge_output.recall, 4),round(rouge_output.fmeasure, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:07:01.178171Z",
     "start_time": "2022-07-20T02:07:01.045914Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_output = rouge.compute(predictions=results[\"pred_summary\"], references=results[\"intents\"], rouge_types=[\"rouge1\"])[\"rouge1\"].mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:07:01.423750Z",
     "start_time": "2022-07-20T02:07:01.413578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2961, 0.2421, 0.2448)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rouge_output.precision, 4),round(rouge_output.recall, 4),round(rouge_output.fmeasure, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:07:02.203407Z",
     "start_time": "2022-07-20T02:07:02.079162Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_output = rouge.compute(predictions=results[\"pred_summary\"], references=results[\"intents\"], rouge_types=[\"rougeL\"])[\"rougeL\"].mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T02:07:02.375389Z",
     "start_time": "2022-07-20T02:07:02.365432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2931, 0.2401, 0.2422)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rouge_output.precision, 4),round(rouge_output.recall, 4),round(rouge_output.fmeasure, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 0.0001\n",
    "(0.282, 0.2271, 0.2327)\n",
    "(0.0731, 0.0654, 0.0659)\n",
    "(0.2798, 0.2257, 0.231)\n",
    "\n",
    "#lr = 0.00005 5e-5\n",
    "(0.2854, 0.2317, 0.2372)\n",
    "(0.0786, 0.0699, 0.0707)\n",
    "(0.2814, 0.2288, 0.2342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T09:14:55.009747Z",
     "start_time": "2022-05-07T09:14:54.991131Z"
    }
   },
   "outputs": [],
   "source": [
    "# idx_intent = np.load('./total_idx_intent.npy', allow_pickle=True).item()\n",
    "# idx_titles = np.load('./total_idx_titles.npy', allow_pickle=True).item()\n",
    "\n",
    "all_bundle = np.load('data/food_evaluation_replace.npy', allow_pickle=True).item()\n",
    "\n",
    "intents = []\n",
    "titles = []\n",
    "for k,v in all_bundle.items():\n",
    "    intents.append(v[2])\n",
    "    titles.append(v[1])\n",
    "    \n",
    "# split train, test set = 8:2\n",
    "# test_num = int(len(intents)*0.2)\n",
    "\n",
    "# train_intent = intents[:-test_num]\n",
    "# train_titles = titles[:-test_num]\n",
    "# test_intent = intents[-test_num:]\n",
    "# test_titles = titles[-test_num:]\n",
    "\n",
    "# title_intent = defaultdict(list)\n",
    "# # encoder_max_length=512\n",
    "# # decoder_max_length=32\n",
    "\n",
    "# for i in range(len(train_intent)):\n",
    "#     # token_titles = tokenizer(train_titles[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=encoder_max_length)\n",
    "#     # title_intent['titles'].append(torch.as_tensor(token_titles.input_ids, dtype=torch.int))\n",
    "#     # title_intent['attention_mask'].append(torch.as_tensor(token_titles.attention_mask, dtype=torch.int))\n",
    "#     # token_labels = tokenizer(train_intent[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=decoder_max_length)\n",
    "#     # title_intent['labels'].append(torch.as_tensor(token_labels.input_ids, dtype=torch.int))\n",
    "#     # title_intent['decoder_attention_mask'].append(torch.as_tensor(token_labels.attention_mask, dtype=torch.int))\n",
    "#     title_intent['titles'].append(train_titles[i])\n",
    "#     title_intent['intents'].append(train_intent[i])\n",
    "    \n",
    "test_title_intent = defaultdict(list)\n",
    "for i in range(len(intents)):\n",
    "    # token_titles = tokenizer(test_titles[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=encoder_max_length)\n",
    "    # test_title_intent['titles'].append(torch.as_tensor(token_titles.input_ids, dtype=torch.int))\n",
    "    # test_title_intent['attention_mask'].append(torch.as_tensor(token_titles.attention_mask, dtype=torch.int))\n",
    "    # token_labels = tokenizer(test_intent[i], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",truncation=True, max_length=decoder_max_length)\n",
    "    # test_title_intent['labels'].append(torch.as_tensor(token_labels.input_ids, dtype=torch.int))\n",
    "    # test_title_intent['decoder_attention_mask'].append(torch.as_tensor(token_labels.attention_mask, dtype=torch.int))\n",
    "    test_title_intent['titles'].append(titles[i])\n",
    "    test_title_intent['intents'].append(intents[i])\n",
    "    \n",
    "# dataset = Dataset.from_dict(title_intent)\n",
    "vali_dataset = Dataset.from_dict(test_title_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T09:15:08.172358Z",
     "start_time": "2022-05-07T09:15:08.064073Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7c3c71ff6301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinalresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_summary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfinalresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_summary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bart_food_replace.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "finalresults = []\n",
    "for i in range(len(results[\"pred_summary\"])):\n",
    "    finalresults.append((results[\"pred_summary\"][i], results[\"intents\"][i]))\n",
    "np.save('bart_food_replace.npy', finalresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
