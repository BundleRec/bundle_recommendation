{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffcc1252-d688-430a-8f20-6b3d4abd6788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fkd/.conda/envs/fkd/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.9.crossattention.bias', 'h.10.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.8.crossattention.masked_bias', 'h.5.crossattention.c_attn.weight', 'h.2.ln_cross_attn.weight', 'h.8.crossattention.bias', 'h.3.crossattention.c_proj.bias', 'h.5.crossattention.masked_bias', 'h.3.crossattention.bias', 'h.4.crossattention.c_proj.bias', 'h.2.crossattention.bias', 'h.1.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.6.crossattention.bias', 'h.1.crossattention.masked_bias', 'h.7.crossattention.masked_bias', 'h.4.crossattention.masked_bias', 'h.6.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.masked_bias', 'h.9.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.bias', 'h.11.crossattention.masked_bias', 'h.2.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.3.ln_cross_attn.weight', 'h.1.crossattention.bias', 'h.7.crossattention.bias', 'h.6.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.9.ln_cross_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.9.crossattention.masked_bias', 'h.6.ln_cross_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.4.crossattention.c_attn.weight', 'h.10.crossattention.bias', 'h.4.crossattention.bias', 'h.11.crossattention.q_attn.weight', 'h.10.ln_cross_attn.weight', 'h.5.ln_cross_attn.weight', 'h.11.ln_cross_attn.weight', 'h.6.crossattention.masked_bias', 'h.0.crossattention.masked_bias', 'h.3.crossattention.c_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.7.ln_cross_attn.weight', 'h.11.crossattention.bias', 'h.7.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.2.crossattention.masked_bias', 'h.5.crossattention.bias', 'h.0.crossattention.bias', 'h.3.crossattention.q_attn.weight', 'h.4.ln_cross_attn.weight', 'h.8.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor, AutoTokenizer, ViTFeatureExtractor\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    \n",
    "\n",
    "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "text_decode_model = \"gpt2\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(image_encoder_model, text_decode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210da4a4-2d55-4f8d-b72b-84b3c6d2fb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vit-gpt-model/tokenizer_config.json',\n",
       " 'vit-gpt-model/special_tokens_map.json',\n",
       " 'vit-gpt-model/vocab.json',\n",
       " 'vit-gpt-model/merges.txt',\n",
       " 'vit-gpt-model/added_tokens.json',\n",
       " 'vit-gpt-model/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",
    "# text tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_decode_model)\n",
    "\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "output_dir = \"vit-gpt-model\"\n",
    "model.save_pretrained(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7c3d5d-181d-4aa5-8e37-4fc82f6d19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "all_files = os.listdir('./data/image')\n",
    "\n",
    "# extract all idx\n",
    "\n",
    "img_idx = []\n",
    "\n",
    "for files in all_files:\n",
    "    pre_suf_fix = files.split('.')\n",
    "    if pre_suf_fix[-1] == 'jpg':\n",
    "        img_idx.append(int(pre_suf_fix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00bf9cc5-4185-448e-9f93-a4b5e270cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "# train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "idx_intent = np.load('./data/total_idx_intent.npy', allow_pickle=True).item()\n",
    "# idx_titles = np.load('./total_idx_titles.npy', allow_pickle=True).item()\n",
    "\n",
    "intents = []\n",
    "img_urls = []\n",
    "for idx in img_idx:\n",
    "    intents.append(idx_intent[idx])\n",
    "    url_str = 'data/image/' + str(idx) + '.jpg'\n",
    "    img_urls.append(url_str)\n",
    "# for k,v in idx_intent.items():\n",
    "#     intents.append(v)\n",
    "#     titles.append(idx_titles[k])\n",
    "    \n",
    "# split train, test set = 8:2\n",
    "test_num = int(len(intents)*0.2)\n",
    "\n",
    "train_intent = intents[:-test_num]\n",
    "train_titles = img_urls[:-test_num]\n",
    "test_intent = intents[-test_num:]\n",
    "test_titles = img_urls[-test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb7e5d9c-825a-4c51-9bec-985a680e2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "\n",
    "image_caption = defaultdict(list)\n",
    "\n",
    "for i in range(len(train_intent)):\n",
    "    image_caption['image'].append(train_titles[i])\n",
    "    image_caption['caption'].append(train_intent[i])\n",
    "    \n",
    "image_caption_test = defaultdict(list)\n",
    "\n",
    "for i in range(len(test_intent)):\n",
    "    image_caption_test['image'].append(test_titles[i])\n",
    "    image_caption_test['caption'].append(test_intent[i])\n",
    "    \n",
    "dataset = Dataset.from_dict(image_caption)\n",
    "dataset_vali = Dataset.from_dict(image_caption_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8466a47-16b7-4754-bd38-e5fe33702d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# text preprocessing step\n",
    "def tokenization_fn(captions, max_target_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(captions, \n",
    "                      padding=\"max_length\", \n",
    "                      max_length=max_target_length).input_ids\n",
    "\n",
    "    return labels\n",
    "\n",
    "# image preprocessing step\n",
    "def feature_extraction_fn(image_paths, check_image=True):\n",
    "    \"\"\"\n",
    "    Run feature extraction on images\n",
    "    If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n",
    "    Otherwise, an exception will be thrown.\n",
    "    \"\"\"\n",
    "\n",
    "    model_inputs = {}\n",
    "\n",
    "    if check_image:\n",
    "        images = []\n",
    "        to_keep = []\n",
    "        for image_file in image_paths:\n",
    "            try:\n",
    "                img = Image.open(image_file)\n",
    "                images.append(img)\n",
    "                to_keep.append(True)\n",
    "            except Exception:\n",
    "                to_keep.append(False)\n",
    "    else:\n",
    "        images = [Image.open(image_file) for image_file in image_paths]\n",
    "\n",
    "    encoder_inputs = feature_extractor(images=images, return_tensors=\"np\")\n",
    "\n",
    "    return encoder_inputs.pixel_values\n",
    "\n",
    "def preprocess_fn(examples, max_target_length, check_image = True):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    image_paths = examples['image']\n",
    "    captions = examples['caption']    \n",
    "    \n",
    "    model_inputs = {}\n",
    "    # This contains image path column\n",
    "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
    "    model_inputs['pixel_values'] = feature_extraction_fn(image_paths, check_image=check_image)\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e4ce1ac-14d8-4abc-858a-5243e9797d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_fn at 0x7feb3d630820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:27<00:00,  5.53s/ba]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.28s/ba]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = dataset.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 128},\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "processed_dataset_test = dataset_vali.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 128},\n",
    "    remove_columns=dataset_vali.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b71a25-6fc4-48c7-8972-75742e04fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./image-captioning-output\",\n",
    "    # learning_rate=7e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecea582-65f4-456f-80af-3d3b35c08b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = datasets.load_metric(\"./rouge.py\")\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    return {\n",
    "        \"rouge2_precision\": round(result.precision, 4),\n",
    "        \"rouge2_recall\": round(result.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(result.fmeasure, 4),\n",
    "    }\n",
    "    # result = metric.compute(predictions=decoded_preds,\n",
    "    #                         references=decoded_labels,\n",
    "    #                         use_stemmer=True)\n",
    "    # result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    # prediction_lens = [\n",
    "    #     np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    # ]\n",
    "    # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a2e2c5-f0e0-49a6-a8e7-51439692725a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fkd/.conda/envs/fkd/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4278\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1605\n",
      "/home/fkd/.conda/envs/fkd/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1605' max='1605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1605/1605 27:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.150629</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.148411</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.149555</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.047900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./image-captioning-output/checkpoint-500\n",
      "Configuration saved in ./image-captioning-output/checkpoint-500/config.json\n",
      "Model weights saved in ./image-captioning-output/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in ./image-captioning-output/checkpoint-500/preprocessor_config.json\n",
      "/home/fkd/.conda/envs/fkd/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1069\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./image-captioning-output/checkpoint-1000\n",
      "Configuration saved in ./image-captioning-output/checkpoint-1000/config.json\n",
      "Model weights saved in ./image-captioning-output/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in ./image-captioning-output/checkpoint-1000/preprocessor_config.json\n",
      "/home/fkd/.conda/envs/fkd/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1069\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./image-captioning-output/checkpoint-1500\n",
      "Configuration saved in ./image-captioning-output/checkpoint-1500/config.json\n",
      "Model weights saved in ./image-captioning-output/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in ./image-captioning-output/checkpoint-1500/preprocessor_config.json\n",
      "/home/fkd/.conda/envs/fkd/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1069\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1605, training_loss=0.14426316412809853, metrics={'train_runtime': 1644.6248, 'train_samples_per_second': 7.804, 'train_steps_per_second': 0.976, 'total_flos': 2.316073578635723e+18, 'train_loss': 0.14426316412809853, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset,\n",
    "    eval_dataset=processed_dataset_test,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79d23a2-7798-4a21-a9a0-86e4251ecfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./image-captioning-output\n",
      "Configuration saved in ./image-captioning-output/config.json\n",
      "Model weights saved in ./image-captioning-output/pytorch_model.bin\n",
      "Feature extractor saved in ./image-captioning-output/preprocessor_config.json\n",
      "tokenizer config file saved in ./image-captioning-output/tokenizer_config.json\n",
      "Special tokens file saved in ./image-captioning-output/special_tokens_map.json\n",
      "Feature extractor saved in ./image-captioning-output/preprocessor_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./image-captioning-output/preprocessor_config.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./image-captioning-output\")\n",
    "tokenizer.save_pretrained(\"./image-captioning-output\")\n",
    "feature_extractor.save_pretrained('./image-captioning-output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457ebdd5-8ee7-4f1f-8ebb-4b131c553781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor, AutoTokenizer, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"./image-captioning-output\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"./image-captioning-output\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./image-captioning-output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ac7d93-c8e5-40cc-b03b-1beb65163c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "# feature_extractor = feature_extractor.to(device)\n",
    "# tokenizer = tokenizer.to(device)\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ef53d5-ec7a-43ea-a9a8-5948d0780e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28f46abe-0384-4723-ba86-1169f8e62fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['different style of earings']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(['./data/image/4923.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6de28e33-99e1-4118-93ec-21148e65c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloth_idx = [5122, 5133, 5143, 5212, 5278,5302,5127,5128, 5177,5230,5321,5210,5342,5379,5203,5296,5333,5405,5373,5434,5155,5144,5367]\n",
    "elec_idx = [5169,5214,5306,5310,5345,5120, 5131, 5292,5390,5117,4976,5203,5416,5089,4925,5293,5349,5354,5370,4976,5346,5208,5149,4923]\n",
    "food_idx = [5198,5239,5323,5341,5120,5192,5241,5110,5161,5294,5362,5366,5386,5411,5180,5233,5364,5176,5125,5398,5351,5392]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "065254b6-11ac-4485-a874-8952e0219508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions_url = []\n",
    "for idx in food_idx:\n",
    "    url = f'./data/image/{str(idx)}.jpg'\n",
    "    predictions_url.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "000547e8-f957-4286-9b6d-9d35e7f8c78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Different food of snack',\n",
       " 'Different food for cooking',\n",
       " 'Different food for cooking',\n",
       " 'Different food for drinking',\n",
       " 'Different types of plugs',\n",
       " 'tablet and accessories',\n",
       " 'snack food',\n",
       " 'Different food of snack',\n",
       " 'Different food of snack',\n",
       " 'Different food for cooking',\n",
       " 'Different food for cooking',\n",
       " 'different brands of coffee',\n",
       " 'Different food of snack',\n",
       " 'Different food for cooking',\n",
       " 'Different food of snack',\n",
       " 'Different food of snack',\n",
       " 'Different food of snack',\n",
       " 'Different food of snack',\n",
       " 'different style of candy',\n",
       " 'Different food of snack',\n",
       " 'Coffee',\n",
       " 'Different food for cooking']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(predictions_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe7b10d-5c85-4501-817c-075fa7035580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafc44a-2f4c-4ab4-bdb3-f2ba8a46059a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
