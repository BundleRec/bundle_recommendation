{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc1252-d688-430a-8f20-6b3d4abd6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor, AutoTokenizer, ViTFeatureExtractor\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    \n",
    "\n",
    "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "text_decode_model = \"gpt2\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(image_encoder_model, text_decode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210da4a4-2d55-4f8d-b72b-84b3c6d2fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",
    "# text tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_decode_model)\n",
    "\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "output_dir = \"vit-gpt-model\"\n",
    "model.save_pretrained(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7c3d5d-181d-4aa5-8e37-4fc82f6d19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "all_files = os.listdir('./data/image')\n",
    "\n",
    "# extract all idx\n",
    "\n",
    "img_idx = []\n",
    "\n",
    "for files in all_files:\n",
    "    pre_suf_fix = files.split('.')\n",
    "    if pre_suf_fix[-1] == 'jpg':\n",
    "        img_idx.append(int(pre_suf_fix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00bf9cc5-4185-448e-9f93-a4b5e270cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "data_dir = 'data/clothing/'\n",
    "\n",
    "idx_intent = np.load(f'{data_dir}total_idx_intent.npy', allow_pickle=True).item()\n",
    "\n",
    "intents = []\n",
    "img_urls = []\n",
    "for idx in img_idx:\n",
    "    intents.append(idx_intent[idx])\n",
    "    url_str = f'{data_dir}image/' + str(idx) + '.jpg'\n",
    "    img_urls.append(url_str)\n",
    "    \n",
    "# split train, test set = 8:2\n",
    "test_num = int(len(intents)*0.2)\n",
    "\n",
    "train_intent = intents[:-test_num]\n",
    "train_titles = img_urls[:-test_num]\n",
    "test_intent = intents[-test_num:]\n",
    "test_titles = img_urls[-test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb7e5d9c-825a-4c51-9bec-985a680e2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "\n",
    "image_caption = defaultdict(list)\n",
    "\n",
    "for i in range(len(train_intent)):\n",
    "    image_caption['image'].append(train_titles[i])\n",
    "    image_caption['caption'].append(train_intent[i])\n",
    "    \n",
    "image_caption_test = defaultdict(list)\n",
    "\n",
    "for i in range(len(test_intent)):\n",
    "    image_caption_test['image'].append(test_titles[i])\n",
    "    image_caption_test['caption'].append(test_intent[i])\n",
    "    \n",
    "dataset = Dataset.from_dict(image_caption)\n",
    "dataset_vali = Dataset.from_dict(image_caption_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8466a47-16b7-4754-bd38-e5fe33702d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# text preprocessing step\n",
    "def tokenization_fn(captions, max_target_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(captions, \n",
    "                      padding=\"max_length\", \n",
    "                      max_length=max_target_length).input_ids\n",
    "\n",
    "    return labels\n",
    "\n",
    "# image preprocessing step\n",
    "def feature_extraction_fn(image_paths, check_image=True):\n",
    "    \"\"\"\n",
    "    Run feature extraction on images\n",
    "    If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n",
    "    Otherwise, an exception will be thrown.\n",
    "    \"\"\"\n",
    "\n",
    "    model_inputs = {}\n",
    "\n",
    "    if check_image:\n",
    "        images = []\n",
    "        to_keep = []\n",
    "        for image_file in image_paths:\n",
    "            try:\n",
    "                img = Image.open(image_file)\n",
    "                images.append(img)\n",
    "                to_keep.append(True)\n",
    "            except Exception:\n",
    "                to_keep.append(False)\n",
    "    else:\n",
    "        images = [Image.open(image_file) for image_file in image_paths]\n",
    "\n",
    "    encoder_inputs = feature_extractor(images=images, return_tensors=\"np\")\n",
    "\n",
    "    return encoder_inputs.pixel_values\n",
    "\n",
    "def preprocess_fn(examples, max_target_length, check_image = True):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    image_paths = examples['image']\n",
    "    captions = examples['caption']    \n",
    "    \n",
    "    model_inputs = {}\n",
    "    # This contains image path column\n",
    "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
    "    model_inputs['pixel_values'] = feature_extraction_fn(image_paths, check_image=check_image)\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e4ce1ac-14d8-4abc-858a-5243e9797d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_fn at 0x7feb3d630820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:27<00:00,  5.53s/ba]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.28s/ba]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = dataset.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 128},\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "processed_dataset_test = dataset_vali.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 128},\n",
    "    remove_columns=dataset_vali.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b71a25-6fc4-48c7-8972-75742e04fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./image-captioning-output\",\n",
    "    # learning_rate=7e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecea582-65f4-456f-80af-3d3b35c08b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = datasets.load_metric(\"./rouge.py\")\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    return {\n",
    "        \"rouge2_precision\": round(result.precision, 4),\n",
    "        \"rouge2_recall\": round(result.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(result.fmeasure, 4),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2e2c5-f0e0-49a6-a8e7-51439692725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset,\n",
    "    eval_dataset=processed_dataset_test,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d23a2-7798-4a21-a9a0-86e4251ecfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./image-captioning-output\")\n",
    "tokenizer.save_pretrained(\"./image-captioning-output\")\n",
    "feature_extractor.save_pretrained('./image-captioning-output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457ebdd5-8ee7-4f1f-8ebb-4b131c553781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor, AutoTokenizer, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"./image-captioning-output\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"./image-captioning-output\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./image-captioning-output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ac7d93-c8e5-40cc-b03b-1beb65163c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "# feature_extractor = feature_extractor.to(device)\n",
    "# tokenizer = tokenizer.to(device)\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28f46abe-0384-4723-ba86-1169f8e62fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['different style of earings']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(['./data/image/4923.jpg'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
